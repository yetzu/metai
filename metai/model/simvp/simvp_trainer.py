# metai/model/simvp_trainer.py

import subprocess
import os
import sys
import time
import glob
from typing import Any, cast, Dict, Optional, Union, List
import torch
import torch.nn as nn
import torch.nn.functional as F
import lightning as l
from lightning.pytorch.utilities.types import OptimizerLRScheduler

# å¯¼å…¥å®é™…ä¾èµ– (å‡è®¾è¿™äº›ç±»å’Œå‡½æ•°éƒ½å­˜åœ¨äºé¡¹ç›®ä¸­)
from metai.model.core import get_optim_scheduler, timm_schedulers
from .simvp_model import SimVP_Model
from .simvp_loss import HybridLoss


class SimVP(l.LightningModule):
    def __init__(self, **args):
        super(SimVP, self).__init__()
        
        self.save_hyperparameters()
        config: Dict[str, Any] = dict(args)
        
        # 1. æ¨¡å‹åˆå§‹åŒ– (SimVP_Model)
        self.model = self._build_model(config)
        
        # 2. Loss Configuration Setup (HybridLoss å‚æ•°ï¼Œç»Ÿä¸€ä½¿ç”¨ loss_weight_ å‰ç¼€)
        loss_weight_l1 = config.get('loss_weight_l1', 1.0)
        loss_weight_ssim = config.get('loss_weight_ssim', 0.5)
        loss_weight_csi = config.get('loss_weight_csi', 1.0)
        loss_weight_spectral = config.get('loss_weight_spectral', 0.1)
        loss_weight_evo = config.get('loss_weight_evo', 0.5)

        # 3. åˆå§‹åŒ– Loss å‡½æ•°
        self.criterion = HybridLoss(
            l1_weight=loss_weight_l1,
            ssim_weight=loss_weight_ssim,
            csi_weight=loss_weight_csi,
            spectral_weight=loss_weight_spectral,
            evo_weight=loss_weight_evo
        )
        
        rs = config.get('resize_shape', None)
        self.resize_shape = tuple(rs) if rs is not None else None

        # è¯¾ç¨‹å­¦ä¹ é…ç½®
        self.use_curriculum_learning = config.get('use_curriculum_learning', True)
        
        # æµ‹è¯•ç›¸å…³é…ç½®
        self.auto_test_after_epoch = config.get('auto_test_after_epoch', True)
        self.test_script_path = config.get('test_script_path', None)
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè„šæœ¬è·¯å¾„ï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾
        if self.test_script_path is None:
            # å°è¯•ä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾è„šæœ¬
            current_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
            script_path = os.path.join(current_dir, 'run.scwds.simvp.sh')
            if os.path.exists(script_path):
                self.test_script_path = script_path
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
                self.test_script_path = 'run.scwds.simvp.sh'
    
    def _build_model(self, config: Dict[str, Any]):
        """å®ä¾‹åŒ– SimVP æ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„ä¼˜åŒ–å‚æ•°"""
        return SimVP_Model(
             in_shape=config.get('in_shape'), hid_S=config.get('hid_S', 128), 
             hid_T=config.get('hid_T', 512), N_S=config.get('N_S', 4), N_T=config.get('N_T', 12),
             model_type=config.get('model_type', 'mamba'), out_channels=config.get('out_channels', 1),
             mlp_ratio=config.get('mlp_ratio', 8.0), drop=config.get('drop', 0.0), drop_path=config.get('drop_path', 0.1),
             spatio_kernel_enc=config.get('spatio_kernel_enc', 3), 
             spatio_kernel_dec=config.get('spatio_kernel_dec', 3)
        )
    
    def configure_optimizers(self) -> OptimizerLRScheduler:
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä½¿ç”¨ metai.model.core"""
        
        max_epochs = getattr(self.hparams, 'max_epochs', 100)
        
        # å‡è®¾ get_optim_scheduler å­˜åœ¨ä¸”å¯ç”¨
        optimizer, scheduler, by_epoch = get_optim_scheduler(self.hparams, max_epochs, self.model)
        
        return cast(OptimizerLRScheduler, {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler, 
                "interval": "epoch" if by_epoch else "step"
            },
        })
    
    def lr_scheduler_step(self, scheduler: Any, metric: Any):
        """å¤„ç† timm è°ƒåº¦å™¨çš„æ­¥è¿›"""
        # å‡è®¾ timm_schedulers å­˜åœ¨ä¸”å¯ç”¨
        if any(isinstance(scheduler, sch) for sch in timm_schedulers):
            scheduler.step(epoch=self.current_epoch)
        else:
            scheduler.step(metric) if metric is not None else scheduler.step()
    
    def on_train_epoch_start(self):
        """è¯¾ç¨‹å­¦ä¹ ï¼šåŠ¨æ€è°ƒæ•´ HybridLoss çš„æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        # å¦‚æœç¦ç”¨è¯¾ç¨‹å­¦ä¹ ï¼Œä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥çš„å›ºå®šæƒé‡ï¼Œä¸è¿›è¡ŒåŠ¨æ€è°ƒæ•´
        if not self.use_curriculum_learning:
            return
        
        epoch = self.current_epoch
        
        # è·å– Loss æ¨¡å— (å‡è®¾ self.criterion æ˜¯ HybridLoss)
        if not hasattr(self, 'criterion') or not isinstance(self.criterion, HybridLoss):
            return

        # === é˜¶æ®µå®šä¹‰ ===
        if epoch < 10: 
            # Phase 1: å®šæ€§ (Warmup)
            weights = {'l1': 5.0, 'ssim': 1.0, 'evo': 0.1, 'spec': 0.0, 'csi': 0.0}
            phase_name = "Phase 1: Qualitative (Structure)"
        
        elif epoch < 30:
            # Phase 2: å®šé‡ (Physics & Sharpness)
            # çº¿æ€§è¿‡æ¸¡ç¤ºä¾‹: evo ä» 0.1 -> 2.0
            progress = (epoch - 10) / 20.0
            evo_w = 0.1 + progress * (2.0 - 0.1)
            spec_w = 0.0 + progress * (0.5 - 0.0)
            csi_w = 0.0 + progress * (1.0 - 0.0)
            
            weights = {'l1': 1.0, 'ssim': 0.5, 'evo': evo_w, 'spec': spec_w, 'csi': csi_w}
            phase_name = f"Phase 2: Quantitative (Physics & Sharpness) [p={progress:.2f}]"
            
        else:
            # Phase 3: å†²åˆº (Score Maximization)
            weights = {'l1': 0.1, 'ssim': 0.2, 'evo': 1.0, 'spec': 1.0, 'csi': 5.0}
            phase_name = "Phase 3: Sprint (Score Maximization)"

        # æ›´æ–°æƒé‡
        self.criterion.weights.update(weights)
        
        # è®°å½•æ—¥å¿—
        if self.trainer.is_global_zero:
            print(f"\n[Curriculum] Epoch {epoch} | Phase: {phase_name}")
            print(f"             Weights: {weights}")
        
        # TensorBoard è®°å½•
        for k, v in weights.items():
            self.log(f"train/weight_{k}", v, on_epoch=True)

    def on_train_epoch_end(self):
        """åœ¨æ¯ä¸ªè®­ç»ƒepochç»“æŸåæ‰§è¡Œæµ‹è¯•è„šæœ¬ï¼ˆåå°æ‰§è¡Œï¼Œä¸é˜»å¡è®­ç»ƒï¼‰"""
        if self.trainer.is_global_zero and self.auto_test_after_epoch:
            try:
                if not self.test_script_path:
                    return
                
                script_path = str(self.test_script_path)
                if not os.path.isabs(script_path):
                    # å°è¯•å®šä½åˆ°é¡¹ç›®æ ¹ç›®å½•
                    current_file = os.path.abspath(__file__)
                    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file))))
                    script_path = os.path.join(project_root, script_path)
                
                if not os.path.exists(script_path):
                    print(f"[WARNING] Test script not found: {script_path}")
                    return
                
                epoch = self.current_epoch
                
                # è·å– checkpoint ä¿å­˜ç›®å½•
                save_dir = None
                if hasattr(self, 'hparams'):
                    # hparams å¯èƒ½æ˜¯ dict æˆ– Namespace
                    if isinstance(self.hparams, dict):
                        save_dir = self.hparams.get('save_dir', None)
                    else:
                        save_dir = getattr(self.hparams, 'save_dir', None)
                
                if save_dir is None:
                    save_dir = getattr(self.trainer, 'default_root_dir', None)
                
                # ç­‰å¾… checkpoint æ–‡ä»¶å‡ºç°
                if save_dir:
                    max_wait_time = 300  # æœ€å¤šç­‰å¾… 5 åˆ†é’Ÿ
                    check_interval = 2  # æ¯ 2 ç§’æ£€æŸ¥ä¸€æ¬¡
                    waited_time = 0
                    ckpt_pattern = os.path.join(save_dir, '*.ckpt')
                    
                    print(f"\n[INFO] Epoch {epoch} done. Waiting for checkpoint in {save_dir}...")
                    
                    while waited_time < max_wait_time:
                        ckpt_files = glob.glob(ckpt_pattern)
                        if len(ckpt_files) > 0:
                            # æ‰¾åˆ°æœ€æ–°çš„ checkpoint
                            latest_ckpt = max(ckpt_files, key=os.path.getmtime)
                            print(f"[INFO] Checkpoint found: {latest_ckpt}")
                            break
                        time.sleep(check_interval)
                        waited_time += check_interval
                        if waited_time % 10 == 0:  # æ¯ 10 ç§’æ‰“å°ä¸€æ¬¡ç­‰å¾…ä¿¡æ¯
                            print(f"[INFO] Still waiting for checkpoint... ({waited_time}s/{max_wait_time}s)")
                    else:
                        print(f"[WARNING] Timeout waiting for checkpoint after {max_wait_time}s. Proceeding anyway...")
                
                # æ—¥å¿—ç›®å½•
                script_dir = os.path.dirname(script_path) or os.getcwd()
                log_dir = os.path.join(script_dir, 'test_logs')
                os.makedirs(log_dir, exist_ok=True)
                
                log_file = os.path.join(log_dir, f'test_epoch_{epoch:03d}.log')
                
                print(f"[INFO] Launching background test: {script_path}")
                
                # åå°æ‰§è¡Œ
                log_fd = open(log_file, 'w')
                try:
                    subprocess.Popen(
                        ['bash', script_path, 'test'], # ä¼ é€’ 'test' å‚æ•°ç»™è„šæœ¬
                        stdout=log_fd,
                        stderr=subprocess.STDOUT,
                        cwd=script_dir,
                        start_new_session=True 
                    )
                    log_fd.close() 
                except Exception as proc_error:
                    if log_fd: log_fd.close()
                    raise proc_error
                
            except Exception as e:
                print(f"[ERROR] Failed to launch test script: {e}")
    
    def forward(self, x):
        return self.model(x)
    
    def _interpolate_batch_gpu(self, batch_tensor: torch.Tensor, mode: str = 'max_pool') -> torch.Tensor:
        """é«˜æ•ˆçš„ GPU æ‰¹é‡æ’å€¼/é™é‡‡æ ·å‡½æ•°"""
        if self.resize_shape is None: return batch_tensor
        T, C, H, W = batch_tensor.shape[1:]
        target_H, target_W = self.resize_shape
        if H == target_H and W == target_W: return batch_tensor
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¸ƒå°”ç±»å‹ï¼Œå¦‚æœæ˜¯åˆ™å…ˆè½¬æ¢ä¸ºæµ®ç‚¹æ•°
        is_bool = batch_tensor.dtype == torch.bool
        if is_bool:
            batch_tensor = batch_tensor.float()
        
        B = batch_tensor.shape[0]
        batch_tensor = batch_tensor.view(B * T, C, H, W)
        
        if mode == 'max_pool':
            processed_tensor = F.adaptive_max_pool2d(batch_tensor, output_size=self.resize_shape) if target_H < H or target_W < W else F.interpolate(batch_tensor, size=self.resize_shape, mode='bilinear', align_corners=False)
        elif mode in ['nearest', 'bilinear']:
            align = False if mode == 'bilinear' else None
            processed_tensor = F.interpolate(batch_tensor, size=self.resize_shape, mode=mode, align_corners=align)
        else:
            raise ValueError(f"Unsupported interpolation mode: {mode}")

        processed_tensor = processed_tensor.view(B, T, C, target_H, target_W)
        
        # å¦‚æœåŸå§‹æ˜¯å¸ƒå°”ç±»å‹ï¼Œè½¬æ¢å›å¸ƒå°”ç±»å‹
        if is_bool:
            processed_tensor = processed_tensor.bool()
        
        return processed_tensor
    
    def training_step(self, batch, batch_idx):
        _, x, y, target_mask, _ = batch
        target_mask = target_mask.bool()

        x = self._interpolate_batch_gpu(x, mode='max_pool')
        y = self._interpolate_batch_gpu(y, mode='max_pool')
        target_mask = self._interpolate_batch_gpu(target_mask, mode='nearest')

        # ğŸš¨ [å…³é”®ä¿®æ­£ 1]: æ¨¡å‹è¾“å‡º Logits Z
        logits_pred = self(x)
        
        # æŸå¤±å‡½æ•°ç°åœ¨ä¼ å…¥ Logits Z
        # HybridLoss å†…éƒ¨ä¼šå¤„ç† Sigmoid å’Œå„é¡¹æŸå¤±è®¡ç®—
        loss, loss_dict = self.criterion(logits_pred, y, mask=target_mask)
        
        # è®°å½•æ€»æŸå¤±
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        
        # è®°å½•å„ä¸ªæŸå¤±ç»„ä»¶ï¼ˆåŸå§‹å€¼å’ŒåŠ æƒå€¼ï¼‰
        loss_components = ['l1', 'ssim', 'csi', 'spec', 'evo']
        for comp in loss_components:
            if comp in loss_dict:
                self.log(f'train_loss_{comp}', loss_dict[comp], on_step=True, on_epoch=True, prog_bar=False)
            if f'{comp}_weighted' in loss_dict:
                self.log(f'train_loss_{comp}_weighted', loss_dict[f'{comp}_weighted'], on_step=True, on_epoch=True, prog_bar=False)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        metadata, x, y, target_mask, input_mask = batch
        target_mask = target_mask.bool()

        x = self._interpolate_batch_gpu(x, mode='max_pool')
        y = self._interpolate_batch_gpu(y, mode='max_pool')
        target_mask = self._interpolate_batch_gpu(target_mask, mode='nearest')
        
        logits_pred = self(x)
        
        # è®¡ç®— Pred (ç”¨äº MAE/MSE æŒ‡æ ‡è®°å½•)
        y_pred = torch.sigmoid(logits_pred)
        y_pred_clamped = torch.clamp(y_pred, 0.0, 1.0)
        
        # æŸå¤±å‡½æ•°ä¼ å…¥ Logits Z
        loss, loss_dict = self.criterion(logits_pred, y, mask=target_mask)
        
        # è®°å½•éªŒè¯é˜¶æ®µçš„å„ä¸ªæŸå¤±ç»„ä»¶
        loss_components = ['l1', 'ssim', 'csi', 'spec', 'evo']
        for comp in loss_components:
            if comp in loss_dict:
                self.log(f'val_loss_{comp}', loss_dict[comp], on_epoch=True, sync_dist=True)
            if f'{comp}_weighted' in loss_dict:
                self.log(f'val_loss_{comp}_weighted', loss_dict[f'{comp}_weighted'], on_epoch=True, sync_dist=True)
        
        # æŒ‡æ ‡è®¡ç®—ä½¿ç”¨ clamped Pred
        mae = F.l1_loss(y_pred_clamped, y)

        # === æ–°å¢ï¼šè®¡ç®—ç®€åŒ–çš„åŠ æƒ TS Score ===
        # åå½’ä¸€åŒ– (å‡è®¾ max=30.0, æ ¹æ®æ‚¨çš„ test ä»£ç )
        MM_MAX = 30.0
        pred_mm = y_pred_clamped * MM_MAX
        target_mm = y * MM_MAX
        
        # é€‰å–å…³é”®é˜ˆå€¼ (å¦‚ç«èµ›è§„åˆ™)
        thresholds = [0.01, 0.1, 1.0, 2.0, 5.0, 8.0] 
        weights = [0.1, 0.1, 0.1, 0.2, 0.2, 0.3] # ç»™äºˆå¼ºé™æ°´æ›´é«˜æƒé‡
        ts_sum = 0.0
        
        for t, w in zip(thresholds, weights):
            # è®¡ç®— TS
            hits = ((pred_mm >= t) & (target_mm >= t)).float().sum()
            misses = ((pred_mm < t) & (target_mm >= t)).float().sum()
            false_alarms = ((pred_mm >= t) & (target_mm < t)).float().sum()
            ts = hits / (hits + misses + false_alarms + 1e-6)
            ts_sum += ts * w
            
        # è®°å½•åŠ æƒ TS ä½œä¸ºéªŒè¯æŒ‡æ ‡ (è¶Šå¤§è¶Šå¥½)
        val_score = ts_sum / sum(weights)

        self.log('val_loss', loss, on_epoch=True, prog_bar=True, sync_dist=True)
        self.log('val_mae', mae, on_epoch=True, sync_dist=True)
        self.log('val_score', val_score, on_epoch=True, prog_bar=True, sync_dist=True)

    def test_step(self, batch, batch_idx):
        metadata, x, y, target_mask, input_mask = batch
        target_mask = target_mask.bool()

        x = self._interpolate_batch_gpu(x, mode='max_pool')
        y = self._interpolate_batch_gpu(y, mode='max_pool')
        target_mask = self._interpolate_batch_gpu(target_mask, mode='nearest')

        # ğŸš¨ [å…³é”®ä¿®æ­£ 3]: æ¨¡å‹è¾“å‡º Logits Z
        logits_pred = self(x)
        y_pred = torch.sigmoid(logits_pred)
        y_pred_clamped = torch.clamp(y_pred, 0.0, 1.0)
        
        with torch.no_grad():
            # æŸå¤±å‡½æ•°ä¼ å…¥ Logits Z
            loss, loss_dict = self.criterion(logits_pred, y, mask=target_mask)
            
            # è®°å½•æµ‹è¯•é˜¶æ®µçš„å„ä¸ªæŸå¤±ç»„ä»¶
            loss_components = ['l1', 'ssim', 'csi', 'spec', 'evo']
            for comp in loss_components:
                if comp in loss_dict:
                    self.log(f'test_loss_{comp}', loss_dict[comp], on_epoch=True)
                if f'{comp}_weighted' in loss_dict:
                    self.log(f'test_loss_{comp}_weighted', loss_dict[f'{comp}_weighted'], on_epoch=True)
            
        try:
            self.log('test_loss', loss, on_epoch=True)
        except RuntimeError:
            pass
        
        result = {
            # è¾“å‡ºä»ä½¿ç”¨ [0, 1] èŒƒå›´çš„é¢„æµ‹å€¼
            'inputs': x[0].cpu().float().numpy(),
            'preds': y_pred_clamped[0].cpu().float().numpy(),
            'trues': y[0].cpu().float().numpy()
        }
        
        return result
    
    def infer_step(self, batch, batch_idx):
        metadata, x, input_mask = batch 
        
        x = self._interpolate_batch_gpu(x, mode='max_pool')
        # ğŸš¨ [å…³é”®ä¿®æ­£ 4]: æ¨ç†æ—¶è¾“å‡º Pred
        logits_pred = self(x)
        y_pred = torch.sigmoid(logits_pred)
        return torch.clamp(y_pred, 0.0, 1.0)

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        return self.infer_step(batch, batch_idx)