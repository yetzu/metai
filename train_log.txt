nohup: ignoring input
--------------------------------------------------------
üöÄ [4x A800] ÂºÄÂßãËÆ≠ÁªÉ MeteoMamba Âü∫Â∫ßÊ®°Âûã (BF16 Mixed)...
--------------------------------------------------------
Seed set to 42
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
/home/yyj/opt/anaconda3/envs/metai/lib/python3.11/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name      | Type       | Params | Mode 
-------------------------------------------------
0 | model     | MeteoMamba | 9.2 M  | train
1 | criterion | HybridLoss | 0      | train
-------------------------------------------------
9.2 M     Trainable params
0         Non-trainable params
9.2 M     Total params
36.974    Total estimated model params size (MB)
248       Modules in train mode
0         Modules in eval mode
[2025-11-26 22:48:28.537] [INFO] ‰ªéÈÖçÁΩÆÊñá‰ª∂Âä†ËΩΩÈÖçÁΩÆ: /home/yyj/code/metai/config.yaml (is_debug=False)
[2025-11-26 22:48:28.986] [INFO] Dataset split: Train=15160, Val=1895, Test=1895
Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking: |          | 0/? [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]NCCL version 2.27.3+cuda12.9
Sanity Checking DataLoader 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  0.15it/s]Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  0.27it/s]                                                                           Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3790 [00:00<?, ?it/s]/home/yyj/opt/anaconda3/envs/metai/lib/python3.11/site-packages/torch/autograd/graph.py:829: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1024, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [1024, 1, 3, 3], strides() = [9, 9, 3, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:334.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0:   0%|          | 1/3790 [00:22<23:46:22,  0.04it/s]Epoch 0:   0%|          | 1/3790 [00:22<23:46:25,  0.04it/s, v_num=8, train_loss_step=0.523]Epoch 0:   0%|          | 2/3790 [00:24<13:06:08,  0.08it/s, v_num=8, train_loss_step=0.523]Epoch 0:   0%|          | 2/3790 [00:25<13:20:07,  0.08it/s, v_num=8, train_loss_step=0.196]Epoch 0:   0%|          | 3/3790 [00:28<9:50:32,  0.11it/s, v_num=8, train_loss_step=0.196] Epoch 0:   0%|          | 3/3790 [00:28<10:03:19,  0.10it/s, v_num=8, train_loss_step=0.113]Epoch 0:   0%|          | 4/3790 [00:31<8:13:57,  0.13it/s, v_num=8, train_loss_step=0.113] Epoch 0:   0%|          | 4/3790 [00:31<8:24:15,  0.13it/s, v_num=8, train_loss_step=0.170]Epoch 0:   0%|          | 5/3790 [00:34<7:11:35,  0.15it/s, v_num=8, train_loss_step=0.170]Epoch 0:   0%|          | 5/3790 [00:34<7:19:49,  0.14it/s, v_num=8, train_loss_step=0.095]Epoch 0:   0%|          | 6/3790 [00:36<6:28:32,  0.16it/s, v_num=8, train_loss_step=0.095]Epoch 0:   0%|          | 6/3790 [00:37<6:35:25,  0.16it/s, v_num=8, train_loss_step=0.114]Epoch 0:   0%|          | 7/3790 [00:39<5:57:48,  0.18it/s, v_num=8, train_loss_step=0.114]Epoch 0:   0%|          | 7/3790 [00:40<6:03:40,  0.17it/s, v_num=8, train_loss_step=0.0969]Epoch 0:   0%|          | 8/3790 [00:42<5:34:42,  0.19it/s, v_num=8, train_loss_step=0.0969]Epoch 0:   0%|          | 8/3790 [00:43<5:39:52,  0.19it/s, v_num=8, train_loss_step=0.0972]Epoch 0:   0%|          | 9/3790 [00:45<5:16:45,  0.20it/s, v_num=8, train_loss_step=0.0972]Epoch 0:   0%|          | 9/3790 [00:45<5:21:19,  0.20it/s, v_num=8, train_loss_step=0.0432]Epoch 0:   0%|          | 10/3790 [00:47<5:02:23,  0.21it/s, v_num=8, train_loss_step=0.0432]Epoch 0:   0%|          | 10/3790 [00:48<5:06:31,  0.21it/s, v_num=8, train_loss_step=0.0834]Epoch 0:   0%|          | 11/3790 [00:50<4:50:39,  0.22it/s, v_num=8, train_loss_step=0.0834]Epoch 0:   0%|          | 11/3790 [00:51<4:54:25,  0.21it/s, v_num=8, train_loss_step=0.0671]Epoch 0:   0%|          | 12/3790 [00:53<4:40:52,  0.22it/s, v_num=8, train_loss_step=0.0671]Epoch 0:   0%|          | 12/3790 [00:54<4:44:20,  0.22it/s, v_num=8, train_loss_step=0.061] Epoch 0:   0%|          | 13/3790 [00:56<4:32:35,  0.23it/s, v_num=8, train_loss_step=0.061]Epoch 0:   0%|          | 13/3790 [00:56<4:35:46,  0.23it/s, v_num=8, train_loss_step=0.0539]Epoch 0:   0%|          | 14/3790 [00:59<4:25:28,  0.24it/s, v_num=8, train_loss_step=0.0539]Epoch 0:   0%|          | 14/3790 [00:59<4:28:25,  0.23it/s, v_num=8, train_loss_step=0.0606]Epoch 0:   0%|          | 15/3790 [01:01<4:19:19,  0.24it/s, v_num=8, train_loss_step=0.0606]Epoch 0:   0%|          | 15/3790 [01:02<4:22:03,  0.24it/s, v_num=8, train_loss_step=0.0672]Epoch 0:   0%|          | 16/3790 [01:04<4:13:53,  0.25it/s, v_num=8, train_loss_step=0.0672]Epoch 0:   0%|          | 16/3790 [01:05<4:16:28,  0.25it/s, v_num=8, train_loss_step=0.0432]Epoch 0:   0%|          | 17/3790 [01:07<4:09:07,  0.25it/s, v_num=8, train_loss_step=0.0432]Epoch 0:   0%|          | 17/3790 [01:08<4:11:32,  0.25it/s, v_num=8, train_loss_step=0.0841]Epoch 0:   0%|          | 18/3790 [01:10<4:04:52,  0.26it/s, v_num=8, train_loss_step=0.0841]Epoch 0:   0%|          | 18/3790 [01:10<4:07:09,  0.25it/s, v_num=8, train_loss_step=0.0539]Epoch 0:   1%|          | 19/3790 [01:12<4:01:02,  0.26it/s, v_num=8, train_loss_step=0.0539]Epoch 0:   1%|          | 19/3790 [01:13<4:03:12,  0.26it/s, v_num=8, train_loss_step=0.051] Epoch 0:   1%|          | 20/3790 [01:15<3:57:32,  0.26it/s, v_num=8, train_loss_step=0.051]Epoch 0:   1%|          | 20/3790 [01:16<3:59:35,  0.26it/s, v_num=8, train_loss_step=0.0371]Epoch 0:   1%|          | 21/3790 [01:18<3:54:25,  0.27it/s, v_num=8, train_loss_step=0.0371]Epoch 0:   1%|          | 21/3790 [01:19<3:56:22,  0.27it/s, v_num=8, train_loss_step=0.0608]Epoch 0:   1%|          | 22/3790 [01:21<3:51:35,  0.27it/s, v_num=8, train_loss_step=0.0608]Epoch 0:   1%|          | 22/3790 [01:21<3:53:27,  0.27it/s, v_num=8, train_loss_step=0.0244]Epoch 0:   1%|          | 23/3790 [01:23<3:49:00,  0.27it/s, v_num=8, train_loss_step=0.0244]Epoch 0:   1%|          | 23/3790 [01:24<3:50:48,  0.27it/s, v_num=8, train_loss_step=0.0885]Epoch 0:   1%|          | 24/3790 [01:26<3:46:39,  0.28it/s, v_num=8, train_loss_step=0.0885]Epoch 0:   1%|          | 24/3790 [01:27<3:48:22,  0.27it/s, v_num=8, train_loss_step=0.0316]Epoch 0:   1%|          | 25/3790 [01:29<3:44:28,  0.28it/s, v_num=8, train_loss_step=0.0316]Epoch 0:   1%|          | 25/3790 [01:30<3:46:07,  0.28it/s, v_num=8, train_loss_step=0.0323]Epoch 0:   1%|          | 26/3790 [01:32<3:42:27,  0.28it/s, v_num=8, train_loss_step=0.0323]Epoch 0:   1%|          | 26/3790 [01:32<3:44:01,  0.28it/s, v_num=8, train_loss_step=0.0574]Epoch 0:   1%|          | 27/3790 [01:34<3:40:33,  0.28it/s, v_num=8, train_loss_step=0.0574]Epoch 0:   1%|          | 27/3790 [01:35<3:42:04,  0.28it/s, v_num=8, train_loss_step=0.0479]Epoch 0:   1%|          | 28/3790 [01:37<3:38:47,  0.29it/s, v_num=8, train_loss_step=0.0479]Epoch 0:   1%|          | 28/3790 [01:38<3:40:15,  0.28it/s, v_num=8, train_loss_step=0.0867]Epoch 0:   1%|          | 29/3790 [01:40<3:37:09,  0.29it/s, v_num=8, train_loss_step=0.0867]Epoch 0:   1%|          | 29/3790 [01:41<3:38:34,  0.29it/s, v_num=8, train_loss_step=0.0477]Epoch 0:   1%|          | 30/3790 [01:43<3:35:37,  0.29it/s, v_num=8, train_loss_step=0.0477]Epoch 0:   1%|          | 30/3790 [01:43<3:36:59,  0.29it/s, v_num=8, train_loss_step=0.0345]